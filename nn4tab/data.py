# AUTOGENERATED! DO NOT EDIT! File to edit: 01_data.ipynb (unless otherwise specified).

__all__ = ['TabularProc', 'Normalize', 'FillMissing', 'Categorify', 'ProcPipeline', 'cont_cat_split', 'TabularDataset',
           'get_dsets', 'get_dl']

# Cell
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.model_selection import train_test_split

import torch
from torch import nn
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader

import gc
import typing
from pathlib import Path
from math import isclose
from typing import Sequence, Union, Tuple

# Cell
from .test_utils import fake_data, test_normalized, test_categorical, test_nans, test_df_processed

# Cell
class TabularProc():
    _order = 1
    isset = False
    def setup(self): pass
    def checkup(self):
        pass
    def encode(self, x):
        raise NotImplementedError
    def decode(self, x): pass

# Cell
def _readargs(**kwargs):
    ds = kwargs.get('ds', None)
    if ds is not None:
        return vars(ds)
    df = kwargs.get('df', None)
    if df is None:
        raise RuntimeError("Either dataset or dataframe should be in arguments")
    cont_names = kwargs.get('cont_names', None)
    cat_names = kwargs.get('cat_names', None)
    return {'data':df,
            'cont_names':cont_names,
            'cat_names':cat_names}

# Cell
class Normalize(TabularProc):
    """
    Normalizes continuous features to zero mean and unit variance.
    """
    def setup(self, data:Union[Dataset, pd.DataFrame], cont_names:Sequence=[]):
        """Store mean and std for columns in cont_names"""
        self.checkup()
        data, cont_names = self._argcheck(data, cont_names)
        self.mean = {col: data[col].mean() for col in cont_names}
        self.std = {col: data[col].std() for col in cont_names}
        self.isset = True

    def _argcheck(self, data, cont_names):
        if isinstance(data, Dataset):
            if not cont_names: cont_names = data.cont_names
            data = data.data
        else:
            if not cont_names:
                raise Warning("Given no columns to process")
        return data, cont_names

    def encode_one(self, df:pd.DataFrame, col:str):
        return (df[col] - self.mean[col])/self.std[col]

    def encode(self, data:Union[Dataset, pd.DataFrame], cont_names:Sequence=[]):
        data, cont_names = self._argcheck(data, cont_names)
        if not self.isset: self.setup(data, cont_names)
        for col in cont_names:
            data[col] = self.encode_one(data, col)

    def decode_one(self, df:pd.DataFrame, col:str):
        return df[col]*self.std[col] + self.mean[col]

    def decode(self, data:Union[Dataset, pd.DataFrame], cont_names:Sequence=[]):
        data, cont_names = self._argcheck(data, cont_names)
        for col in cont_names:
            data[col] = self.decode_one(data, col)

# Cell
class FillMissing(TabularProc):
    """Fills missing values in continuous columns"""
    def __init__(self, add_bool=True, method='mean'):
        self.add_bool = add_bool
        self.method = method

    def setup(self, data:Union[Dataset, pd.DataFrame], cont_names:Sequence=[], cat_names:Sequence=[]):
        self.checkup()
        data, cont_names, cat_names = self._argcheck(data, cont_names, cat_names)
        if self.method == 'mean':
            self.values = {col:data[col].mean() for col in cont_names}
        self.cont_names = cont_names
        self.cat_names = cat_names
        self.isset = True

    def _argcheck(self, data, cont_names, cat_names):
        if isinstance(data, Dataset):
            if not cont_names: cont_names = data.cont_names
            if not cat_names: cat_names = data.cat_names
            data = data.data
        else:
            if not cont_names:
                raise Warning("Given no columns to process")
        return data, cont_names,cat_names

    def encode(self, data:Union[Dataset, pd.DataFrame], cont_names:Sequence=[], cat_names:Sequence=[]):
        data, cont_names, cat_names = self._argcheck(data, cont_names, cat_names)
        if not self.isset: self.setup(data, cont_names, cat_names)
        for col in cont_names:
            if data[col].notna().all():
                continue
            if self.add_bool:
                data[f'{col}_na'] = data[col].isna().astype(np.int8)
                # add name to dataset.cat_names
                cat_names.append(f'{col}_na')
            data[col].fillna(value=self.values[col], inplace=True)

    def decode(self, *args, **kwargs):
        pass

# Cell
def _catlist(s:pd.Series):
    c = set(s)
    c.discard('#na')
    return ['#na'] + list(c)

# Cell
class Categorify(TabularProc):
    """Numericalizes categorical columns."""
    def setup(self, data:Union[Dataset, pd.DataFrame], cat_names:Sequence=[]):
        self.checkup()
        data, cat_names = self._argcheck(data, cat_names)
        self.cat = {col: _catlist(data[col].dropna()) for col in cat_names}
        self.i2c = {c: i for i, c in enumerate(self.cat)}
        self.isset = True

    def _argcheck(self, data, cat_names):
        if isinstance(data, Dataset):
            if not cat_names: cat_names = data.cat_names
            data = data.data
        else:
            if not cat_names:
                raise Warning("Given no columns to process")
        return data, cat_names

    def encode_one(self, df:pd.DataFrame, col:str):
        return pd.Series(pd.Categorical(df[col].fillna('#na'), categories=self.cat[col])).cat.codes

    def encode(self, data:Union[Dataset, pd.DataFrame], cat_names:Sequence=[]):
        data, cat_names = self._argcheck(data, cat_names)
        if not self.isset: self.setup(data, cat_names)
        for col in cat_names:
            data[col] = self.encode_one(data, col)

    def decode_one(self, df:pd.DataFrame, col:str):
        return pd.Series(pd.Categorical.from_codes(df[col], categories=self.cat[col]))

    def decode(self, data:Union[Dataset, pd.DataFrame], cat_names:Sequence=[]):
        data, cat_names = self._argcheck(data, cat_names)
        for col in cat_names:
            data[col] = self.decode_one(data, col)

# Cell
class ProcPipeline:
    """
    Combines data processors into pipeline
    """
    def __init__(self, procs:Sequence[TabularProc]):
        self._procs = procs
        self.reset()

    def setup(self, data):
        # todo
        return
        if not self.isset:
            for proc in self.procs:
                proc.setup(data)
        self.isset = True

    def encode(self, data):
        for proc in self.procs:
            proc.encode(data)

    def decode(self, data):
        for proc in self.procs:
            proc.decode(data)

    def __getitem__(self, i):
        return self.procs[i]

    def reset(self):
        procs = [p() for p in self._procs]
        self.procs = sorted(procs, key=lambda p: p._order)
        self.isset = False

# Cell
def cont_cat_split(df, dep_var=None, max_card=np.inf, ignore=[]):
    """
    Sugests a split of columns of the dataframe to continuous and categorical ommiting dep_var and
    ignore. Split is done based on column datatype: float columns and int with cardinality > max_card
    are treated as continuous, all other - categorical.
    """
    cont, cat = [], []
    for col in df.columns:
        if (col == dep_var) or (col in dep_var) or (col in ignore): continue
        if np.issubdtype(df[col].dtype, np.floating) or (len(df[col].unique()) > max_card and np.issubdtype(df[col].dtype, np.integer)):
            cont.append(col)
        else: #?? any condition np.issubdtype(df[col].dtype, np.integer)
            cat.append(col)
    return cont, cat

# Cell
class TabularDataset(Dataset):
    """
    Dataset for continious data.
    Produces tuple containing numpy arrays:
        x_cat, x_cont, y
    """
    def __init__(self, df:pd.DataFrame, cont_names:Sequence, cat_names:Sequence, dep_var:Sequence,
                 procs=[], copy=True):
        self.data = df.copy() if copy else df
        self.cat_names = cat_names
        self.cont_names = cont_names
        self.dep_var = dep_var
        self.procs = procs if isinstance(procs, ProcPipeline) else ProcPipeline(procs)
        self.procs.encode(self)

    def __getitem__(self, idx):
        return (self.data[self.cat_names].iloc[idx].to_numpy(dtype=np.long),
                self.data[self.cont_names].iloc[idx].to_numpy(dtype=np.float32),
                self.data[self.dep_var].iloc[idx].to_numpy(dtype=np.float32))

    def __len__(self):
        return len(self.data)

    def _decode(self):
        self.procs.decode(self)

# Cell
def get_dsets(df:pd.DataFrame, cont_names:Sequence, cat_names:Sequence, dep_var:Sequence,
              procs=[], splits=None, stratify=True):
    if splits:
        train_df, valid_df = df[splits[0]].copy(), df[splits[1]].copy()
    else:
        s = df[dep_var[0]] if stratify else None
        train_df, valid_df = train_test_split(df, test_size=0.2, stratify=s)
    train_df.reset_index(drop=True, inplace=True)
    valid_df.reset_index(drop=True, inplace=True)
    train_ds = TabularDataset(train_df, cont_names, cat_names, dep_var, procs=procs)
    valid_ds = TabularDataset(valid_df, cont_names, cat_names, dep_var, procs=train_ds.procs)
    return (train_ds, valid_ds)

# Cell
def get_dl(ds, bs=512, train=True, drop_last=True):
    return DataLoader(ds, batch_size=bs, shuffle=train, drop_last=drop_last)