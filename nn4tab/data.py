# AUTOGENERATED! DO NOT EDIT! File to edit: 01_data.ipynb (unless otherwise specified).

__all__ = ['cont_cat_split', 'TabularDataset', 'get_dsets', 'get_dl']

# Cell
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.model_selection import train_test_split

import torch
from torch import nn
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader

import gc
import typing
from typing import Sequence, Union, Tuple

# Cell
def cont_cat_split(df, dep_var=None, max_card=np.inf):
    cont, cat = [], []
    for col in df.columns:
        if col == dep_var: continue #?? mb change to support multiple dep var
        if np.issubdtype(df[col].dtype, np.floating) or (len(df[col].unique()) > max_card and np.issubdtype(df[col].dtype, np.integer)):
            cont.append(col)
        else: #?? any condition np.issubdtype(df[col].dtype, np.integer)
            cat.append(col)
    return cont, cat

# Cell
class TabularDataset(Dataset):

    def __init__(self, df:pd.DataFrame, cat_names:Sequence, cont_names:Sequence, dep_var:Sequence):
        self.data = df
        self.cat = cat_names
        self.cont = cont_names
        self.dep_var = dep_var

    def __getitem__(self, idx):
        return (self.data[self.cat].iloc[idx].to_numpy(dtype=np.long),
                self.data[self.cont].iloc[idx].to_numpy(dtype=np.float32),
                self.data[self.dep_var].iloc[idx].to_numpy(dtype=np.float32))

    def __len__(self):
        return len(self.data)

# Cell
def get_dsets(df:pd.DataFrame, cat_names:Sequence, cont_names:Sequence, dep_var:Sequence, splits=None, stratify=True):
    if splits:
        train_df, valid_df = df[splits[0]], df[splits[1]]
    else:
        s = df[dep_var[0]] if stratify else None
        train_df, valid_df = train_test_split(df, test_size=0.2, stratify=s)
    return (TabularDataset(train_df, cat_names, cont_names, dep_var),
            TabularDataset(valid_df, cat_names, cont_names, dep_var))

# Cell
def get_dl(ds, bs=512, train=True, drop_last=True):
    return DataLoader(ds, batch_size=bs, shuffle=train, drop_last=drop_last)