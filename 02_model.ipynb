{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide\n",
    "from nbdev import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import gc\n",
    "import typing\n",
    "from typing import Sequence, Union, Tuple, List"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def noop(x):\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class LinearBlock(nn.Module):\n",
    "    def __init__(self, ni:int, no:int, bn:bool=True, drop:float=0., activation:Union[str, None]='relu'):\n",
    "        super().__init__()\n",
    "        m = []\n",
    "        if bn: m.append(nn.BatchNorm1d(ni))\n",
    "        m.append(nn.Linear(ni, no))\n",
    "        if drop: m.append(nn.Dropout(drop))\n",
    "        if activation=='relu': m.append(nn.ReLU()) \n",
    "        self.m = nn.Sequential(*m)\n",
    "    def forward(self, x):\n",
    "        return self.m(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export \n",
    "## Modified Embedding layer like in fast.ai lib https://github.com/fastai/fastai\n",
    "def trunc_normal_(x, mean=0., std=1.):\n",
    "    \"Truncated normal initialization (approximation)\"\n",
    "    # From https://discuss.pytorch.org/t/implementing-truncated-normal-initializer/4778/12\n",
    "    return x.normal_().fmod_(2).mul_(std).add_(mean)\n",
    "\n",
    "class Embedding(nn.Embedding):\n",
    "    \"Embedding layer with truncated normal initialization\"\n",
    "    def __init__(self, ni, nf, **kwargs):\n",
    "        super().__init__(ni, nf, **kwargs)\n",
    "        trunc_normal_(self.weight.data, std=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class TabInputBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, emb_sz:Sequence[Tuple], n_cont:int, emb_drop:float=0.):\n",
    "        super().__init__()\n",
    "        self.n_cont = n_cont\n",
    "        self.bn = nn.BatchNorm1d(n_cont) if n_cont else noop\n",
    "        self.embs = nn.ModuleList([Embedding(ni, nf) for ni, nf in emb_sz])\n",
    "        self.emb_drop = nn.Dropout(emb_drop) if emb_drop else noop\n",
    "\n",
    "    def forward(self, x_cat, x_cont):\n",
    "        # mb rewrite without python list compr\n",
    "        if self.embs:\n",
    "            x = [e(x_cat[:,i]) for i,e in enumerate(self.embs)]\n",
    "            x = torch.cat(x, 1)\n",
    "            x = self.emb_drop(x)\n",
    "        if self.n_cont != 0:\n",
    "            x_cont = self.bn(x_cont)\n",
    "            x = torch.cat([x, x_cont], 1) if self.embs else x_cont\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class TabularModel(nn.Module):\n",
    "\n",
    "    def __init__(self, layers:Sequence[int], emb_sz:Sequence[Tuple], n_cont:int, n_out:int,\n",
    "                 emb_drop:float=0., drops:Sequence=[]):\n",
    "        super().__init__()\n",
    "        self.stem = TabInputBlock(emb_sz, n_cont, emb_drop)\n",
    "        \n",
    "        layers = [sum([x[1] for x in emb_sz]) + n_cont] + layers\n",
    "        if not drops: drops = [0. for _ in layers]\n",
    "        lins = [LinearBlock(ni, no, drop=p) for ni, no, p in zip(layers[:-1], layers[1:], drops)]\n",
    "        lins.append(LinearBlock(layers[-1], n_out, activation=None))\n",
    "        self.lins = nn.Sequential(*lins)\n",
    "\n",
    "    def forward(self, x_cat, x_cont):\n",
    "        return self.lins(self.stem(x_cat, x_cont))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def get_tabular_model(dataset, n_out, cont_names=[], cat_names=[],  layers=[200,100], emb_voc={}, emb_drop=0.1, drops=[]):\n",
    "    if not (cont_names or cat_names):\n",
    "        cont_names, cat_names = dataset.cont_names, dataset.cat_names\n",
    "    emb_sz = emb_sizes(dataset, cat_names, emb_voc)\n",
    "    return TabularModel(layers, emb_sz, len(cont_names), n_out, emb_drop=emb_drop, drops=drops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "# used in fastai lib\n",
    "def emb_sz_rule(n_cat):\n",
    "    \"Rule of thumb to pick embedding size corresponding to `n_cat`\"\n",
    "    return min(600, round(1.6 * n_cat**0.56))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def emb_sizes(dataset, cat_names, emb_voc={}):\n",
    "    cat_sz = [len(dataset.data[col].unique())+1 for col in cat_names]\n",
    "    emb_sz = [emb_voc.get(col, emb_sz_rule(cat_sz[i])) for i, col in enumerate(cat_names)]\n",
    "    return list(zip(cat_sz, emb_sz))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 00_core.ipynb.\n",
      "Converted 00a_test_utils.ipynb.\n",
      "Converted 01_data.ipynb.\n",
      "Converted 02_model.ipynb.\n",
      "Converted 03_learner.ipynb.\n",
      "Converted index.ipynb.\n"
     ]
    }
   ],
   "source": [
    "# hide\n",
    "from nbdev.export import notebook2script\n",
    "notebook2script()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
