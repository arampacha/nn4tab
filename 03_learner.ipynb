{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from nbdev import *\n",
    "from nbdev.export import notebook2script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "import math\n",
    "# import logging\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import LambdaLR, OneCycleLR\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class AverageMeter:\n",
    "    def __init__(self, store_vals=False, store_avgs=False):\n",
    "        self.store_vals = store_vals\n",
    "        self.store_avgs = store_avgs\n",
    "        if store_vals: self.values = []\n",
    "        if store_avgs: self.avgs = []\n",
    "        self.sum, self.n, self.avg = 0, 0, None\n",
    "        \n",
    "    def update(self, v):\n",
    "        if self.store_vals: self.values.append(v)\n",
    "        self.n += 1\n",
    "        self.sum += v\n",
    "        self.avg = self.sum/self.n\n",
    "        \n",
    "    def reset(self):\n",
    "        if self.store_avgs and self.avg: self.avgs.append(self.avg)\n",
    "        self.sum, self.n, self.avg = 0, 0, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def accuracy_binary(pred, targ):\n",
    "    return ((pred>0).float() == targ).float().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide\n",
    "class Trainer:\n",
    "\n",
    "    def __init__(self, model, train_dataset, test_dataset, config):\n",
    "        self.model = model\n",
    "        self.train_dataset = train_dataset\n",
    "        self.test_dataset = test_dataset\n",
    "        self.config = config\n",
    "\n",
    "        # take over whatever gpus are on the system\n",
    "        self.device = 'cpu'\n",
    "        if torch.cuda.is_available():\n",
    "            self.device = torch.cuda.current_device()\n",
    "            self.model = torch.nn.DataParallel(self.model).to(self.device)\n",
    "\n",
    "    def save_checkpoint(self):\n",
    "        if self.config.ckpt_path is not None:\n",
    "            ckpt_model = self.model.module if hasattr(self.model, \"module\") else self.model\n",
    "            logger.info(\"saving %s\", self.config.ckpt_path)\n",
    "            torch.save(ckpt_model.state_dict(), self.config.ckpt_path)\n",
    "\n",
    "    def train(self):\n",
    "        model, config = self.model, self.config\n",
    "\n",
    "        # create the optimizer\n",
    "        no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
    "        params_decay = [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)]\n",
    "        params_nodecay = [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)]\n",
    "        optim_groups = [\n",
    "            {\"params\": params_decay, \"weight_decay\": config.weight_decay},\n",
    "            {\"params\": params_nodecay, \"weight_decay\": 0.0},\n",
    "        ]\n",
    "        optimizer = optim.AdamW(optim_groups, lr=config.learning_rate, betas=config.betas)\n",
    "\n",
    "        def run_epoch(split):\n",
    "            is_train = split == 'train'\n",
    "            model.train(is_train)\n",
    "            data = self.train_dataset if is_train else self.test_dataset\n",
    "            loader = DataLoader(data, batch_size=config.batch_size, num_workers=config.num_workers)\n",
    "\n",
    "            losses = []\n",
    "            pbar = tqdm(enumerate(loader), total=len(loader)) if is_train else enumerate(loader)\n",
    "            for it, (x, y) in pbar:\n",
    "\n",
    "                # place data on the correct device\n",
    "                x = x.to(self.device)\n",
    "                y = y.to(self.device)\n",
    "\n",
    "                # forward the model\n",
    "                with torch.set_grad_enabled(is_train):\n",
    "                    logits, loss = model(x, y)\n",
    "                    loss = loss.mean() # collapse all losses if they are scattered on multiple gpus\n",
    "                    losses.append(loss.item())\n",
    "\n",
    "                if is_train:\n",
    "\n",
    "                    # backprop and update the parameters\n",
    "                    model.zero_grad()\n",
    "                    loss.backward()\n",
    "                    torch.nn.utils.clip_grad_norm_(model.parameters(), config.grad_norm_clip)\n",
    "                    optimizer.step()\n",
    "\n",
    "                    # decay the learning rate based on our progress\n",
    "                    if config.lr_decay:\n",
    "                        self.tokens += (y >= 0).sum() # number of tokens processed this step (i.e. label is not -100)\n",
    "                        if self.tokens < config.warmup_tokens:\n",
    "                            # linear warmup\n",
    "                            lr_mult = float(self.tokens) / float(max(1, config.warmup_tokens))\n",
    "                        else:\n",
    "                            # cosine learning rate decay\n",
    "                            progress = float(self.tokens - config.warmup_tokens) / float(max(1, config.final_tokens - config.warmup_tokens))\n",
    "                            lr_mult = max(0.1, 0.5 * (1.0 + math.cos(math.pi * progress)))\n",
    "                        lr = config.learning_rate * lr_mult\n",
    "                        for param_group in optimizer.param_groups:\n",
    "                            param_group['lr'] = lr\n",
    "                    else:\n",
    "                        lr = config.learning_rate\n",
    "\n",
    "                    # report progress\n",
    "                    pbar.set_description(f\"epoch {epoch+1} iter {it}: train loss {loss.item():.5f}. lr {lr:e}\")\n",
    "\n",
    "            if not is_train:\n",
    "                logger.info(\"test loss: %f\", np.mean(losses))\n",
    "\n",
    "        self.tokens = 0 # counter used for learning rate decay\n",
    "        for epoch in range(config.max_epochs):\n",
    "\n",
    "            run_epoch('train')\n",
    "            if self.test_dataset is not None:\n",
    "                run_epoch('test')\n",
    "\n",
    "            self.save_checkpoint()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class Callback:\n",
    "    def __init__(self, learn):\n",
    "        self.learn = learn\n",
    "        \n",
    "    def __getattr__(self, attr):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide\n",
    "class TrainEvalCallback(Callback):\n",
    "    \n",
    "    def before_train(self):\n",
    "        self.learn.model.train()\n",
    "        self.learn.training = True\n",
    "    \n",
    "    def before_validate(self):\n",
    "        self.learn.model.eval()\n",
    "        self.learn.training = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide\n",
    "class SaveModelCallback(Callback):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "data_train = np.random.normal(size=(16*100, 11)).astype(np.float32)\n",
    "data_valid = np.random.normal(size=(16*100, 11)).astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DS(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx, :-1], self.data[idx, -1]\n",
    "    def __len__(self):\n",
    "        return len(self.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl = DataLoader(DS(data_train), 16)\n",
    "valid_dl = DataLoader(DS(data_valid), 16)\n",
    "dls = [train_dl, valid_dl]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = next(iter(train_dl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential(nn.Linear(10, 10),\n",
    "                      nn.BatchNorm1d(10),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(10, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 1])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = model(x)\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weight\n",
      "bias\n"
     ]
    }
   ],
   "source": [
    "for n, p in nn.BatchNorm1d(10).named_parameters():\n",
    "    print(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_params(model):\n",
    "    decay, no_decay = [], []\n",
    "    for m in model:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear(in_features=10, out_features=10, bias=True)\n",
      "BatchNorm1d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "ReLU()\n",
      "Linear(in_features=10, out_features=1, bias=True)\n"
     ]
    }
   ],
   "source": [
    "for bla in model:\n",
    "    print(bla)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class LearnerV0:\n",
    "    \n",
    "    def __init__(self, model, dataloaders, opt_func, loss_func, metrics=None, use_gpu=True, savepath='./models'):\n",
    "        \n",
    "        self.device = 'cuda' if (torch.cuda.is_available() and use_gpu) else 'cpu'\n",
    "        self.model = model.to(self.device)\n",
    "        \n",
    "        self.train_dl = dataloaders[0]\n",
    "        self.valid_dl = dataloaders[1]\n",
    "        self.test_dl = dataloaders[2] if len(dataloaders)>2 else None\n",
    "        \n",
    "        self.opt_func = opt_func\n",
    "        self.loss_func = loss_func\n",
    "        self.metrics = metrics\n",
    "        \n",
    "        self.train_losses = AverageMeter(store_vals=True)\n",
    "        self.valid_losses = AverageMeter(store_avgs=True)\n",
    "        self.accs = AverageMeter()\n",
    "#         self.optimizer = opt_func([p for p in self.model.parameters() if p.requires_grad])\n",
    "        \n",
    "        self.savepath = Path(savepath)\n",
    "        if not self.savepath.exists():\n",
    "            self.savepath.mkdir()\n",
    "        self.training = True\n",
    "        self.epoch = -1\n",
    "        \n",
    "    def fit(self, epochs, lr=1e-2):\n",
    "        self.optimizer = self.opt_func([p for p in self.model.parameters() if p.requires_grad], lr)\n",
    "        for e in range(epochs):\n",
    "            self.epoch += 1\n",
    "            train_loss = self.train()\n",
    "            self.train_losses.reset()\n",
    "            \n",
    "            valid_loss, acc = self.validate()\n",
    "            self.valid_losses.reset()\n",
    "            self.accs.reset()\n",
    "            \n",
    "#             print('Train loss = {:f}; valid loss = {:f}; {} = {:f}'.\\\n",
    "#                   format(train_loss, valid_loss, self.metrics.__name__, acc))\n",
    "            self.save_model()\n",
    "        \n",
    "    \n",
    "    def train(self):\n",
    "        \n",
    "        self.model.train()\n",
    "        pbar = tqdm(self.train_dl)\n",
    "        for x_cat, x_cont, y in pbar:\n",
    "            x_cat = x_cat.to(self.device, dtype=torch.long)\n",
    "            x_cont = x_cont.to(self.device)\n",
    "            y = y.to(self.device)\n",
    "\n",
    "            self.optimizer.zero_grad()\n",
    "            pred = self.model(x_cat, x_cont)\n",
    "            loss = self.loss_func(pred, y)\n",
    "\n",
    "            loss.backward()\n",
    "            #torch.nn.utils.clip_grad_norm_(model.parameters(), grad_norm_clip)\n",
    "            self.optimizer.step()\n",
    "\n",
    "            self.train_losses.update(loss.item())\n",
    "            pbar.set_description(f'epoch {self.epoch+1}: train loss {self.train_losses.avg:.4f}')\n",
    "        return self.train_losses.avg\n",
    "    \n",
    "    def validate(self):\n",
    "        \n",
    "        self.model.eval()\n",
    "        pbar = tqdm(self.valid_dl)\n",
    "        for x_cat, x_cont, y in pbar:\n",
    "            x_cat = x_cat.to(self.device, dtype=torch.long)\n",
    "            x_cont = x_cont.to(self.device)\n",
    "            y = y.to(self.device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                pred = self.model(x_cat, x_cont)\n",
    "                loss = self.loss_func(pred, y)\n",
    "\n",
    "            self.valid_losses.update(loss.item())\n",
    "            self.accs.update(accuracy_binary(pred, y).item())\n",
    "            pbar.set_description(f'epoch {self.epoch+1}: valid loss {self.valid_losses.avg:.4f}, accuracy {self.accs.avg :.4f}')\n",
    "        \n",
    "        return self.valid_losses.avg, self.accs.avg\n",
    "    \n",
    "    def save_model(self, fn='ckpt_', path=None):\n",
    "        if not path: path = self.savepath\n",
    "        fn += str(self.epoch) + '.pt'\n",
    "        torch.save(self.model.state_dict(), path/fn)\n",
    "    \n",
    "    def load_model(self, fn, path=None):\n",
    "        if not path: path = self.savepath\n",
    "        self.model.load_state_dict(torch.load(path/fn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide\n",
    "# try:\n",
    "#     self._split(b);                                  self('begin_batch')\n",
    "#     self.pred = self.model(*self.xb);                self('after_pred')\n",
    "#     self.loss = self.loss_func(self.pred, *self.yb); self('after_loss')\n",
    "#     if not self.training: return\n",
    "#     self.loss.backward();                            self('after_backward')\n",
    "#     self.opt.step();                                 self('after_step')\n",
    "#     self.opt.zero_grad()\n",
    "# except CancelBatchException:                         self('after_cancel_batch')\n",
    "# finally:                                             self('after_batch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide\n",
    "def fit(model, data):\n",
    "    get_batch(data)\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iter = iter(train_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n"
     ]
    }
   ],
   "source": [
    "for i in range(1000):\n",
    "    try:\n",
    "        next(train_iter)\n",
    "    except StopIteration:\n",
    "        print(i)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 00_core.ipynb.\n",
      "Converted 01_data.ipynb.\n",
      "Converted 02_model.ipynb.\n",
      "Converted 03_learner.ipynb.\n",
      "Converted index.ipynb.\n"
     ]
    }
   ],
   "source": [
    "notebook2script()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
